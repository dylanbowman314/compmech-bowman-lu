# compmech-bowman-lu

This repository contains the code base for "Unsupervised Recovery of Hidden Markov Models from Transformers with Evolutionary Algorithms", a technical report from Colin Lu and Dylan Bowman for research conducted during the **Computational Mechanics Hackathon** organized by PIBBSS and Simplex from June 1, 2024 to June 3, 2024.

## Abstract

Prior work finds that transformer neural networks trained to mimic the output of a Hidden Markov Model (HMM) embed the optimal Bayesian beliefs for the HMMâ€™s current state in their residual stream, which can be recovered via linear regression. In this work, we aim to address the problem of extracting information about the underlying HMM using the residual stream, without needing to know the MSP already. To do so, we use the R2 of the linear regression as a reward signal for evolutionary algorithms, which are deployed to search for the parameters that generated the source HMM. We find that for toy scenarios where the HMMis generated by a small set of latent variables, the R2 reward signal is remarkably smooth and the evolutionary algorithms succeed in approximately recovering the original HMM. We believe this work constitutes a promising first step towards the ultimate goal of extracting information about the underlying predictive and generative structure of sequences, by analyzing transformers in the wild

## Code

### Setup

Assuming you already have a Python environment set up, clone the `epsilon-transformers` ([GitHub](https://github.com/adamimos/epsilon-transformers)) repository and follow the setup instruction for installation with `pip`. Then, clone this repository.

### Exploratory Analysis

- The `src` folder contains two scripts, `generate_paths_and_beliefs.py` and `experiment.py`. These scripts are meant to be run sequentially.
    - `generate_paths_and_beliefs/py`: For each set of parameters in `src/msp_cfg.yaml`, generate the optimal Bayesian belief states for every possible input sequence. The output tensor is cached in a `.pt` file in `src/cached_belief_store`. `src/cached_belief_store` comes pre-populated with the beliefs from our grid search so there isn't a need to run this script unless you're generating beliefs for new sets of parameters.
    - `experiment.py`: For each set of parameters in `src/msp_cfg.yaml` and for each 


### Evolutionary Algorithms
